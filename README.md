

# Audio Generation with DiffusionLM 

## Introduction 

In recent years, music or audio generation and synthesis have become a sought after challenge in the world of data generation. Current methods of music generation are divided into two channels - (a) capitalizing auto-regressive models to generate music in the continuous domain, and (b) the use of non auto-regressive models to produce music represented in discrete space. Lately, using text-based language models to produce discrete domain music has gained a lot of prominence. Music when represented discretely is a sequence of tokens (symbols) depicting different music information such as pitch, frequency, velocity, bass, and so on, for each musical note at different intervals in time. Leveraging on the sequential nature of the discrete domain music, we propose to adapt a novel state-of-the-art language model, \textit{Diffusion-LM}, to generate piano sounds. 

## Methodology 

### Data Pre-processing and Token Encoding

We used the Lakh Large MIDI dataset, which contains more than 10,000 piano MIDI files to generate piano music. Since parsing the information contained in the MIDI file to an appropriate sequential representation in the text file is an important step in our project, we utilized a token encoding system inspired by the mmmtrack encoding system [12]. 


To fully capture the information from MIDI file, we extract the information regarding pitch, velocity, and duration of each note and parse it to a text file using the encoding- $p\square\square$, $v\square\square$, and $d\square\square$, where $\square$ denotes the placeholder to represent the note's pitch, velocity, and duration, respectively. If there are multiple notes appearing simultaneously, we "compress" them and made them a series of notations. 

Since it is difficult to pre-process MIDI file directly, we pre-process the text file generated by MIDI. In the pre-processing stage, we clamp the note with extremely high or low pitch and add label to denote the beginning and the ending of the text file. 

### Diffusion-LM

Since the original diffusion model is for continuous modeling and the text-generating task is in the discrete space, we need to find the way to map the discrete text to the continuous diffusion model. To address this, proposed an end-to-end training objective for learning word ebeddings. After that, we need to map the word embeddings back to words. proposed a training and decoding time methods to facilitate the mapping function. 

Compared to the random Gaussian embeddings or the pre-trained word embeddings, end-to-end training in [7] is the optimal. We add the text next to the $x_0$ state. In the forward pass, the transition function is $q(x_0|Text) = \mathcal{N}(Embedding(Text), \sigma_0 I )$. In the reverse pass, we add the trainable rounding step, parameterized by $p_{\theta}(Text|x_0) = \prod\limits_{i=1}^{n} p_{\theta}(Text_i|x_i)$, where the last term is the softmax distribution. 

### Result




